version: "3.6"
services:
  # Master
  master-node:
    image: "jwaresolutions/big-data-cluster:0.4.1"
    command: bash -c "/home/big_data/spark-cmd.sh start master-node"
    ports:
      - target: 8088
        published: 8088
        protocol: tcp
        mode: host
      - target: 8080
        published: 8080
        protocol: tcp
        mode: host
      - target: 9870
        published: 9870
        protocol: tcp
        mode: host
      - target: 18080
        published: 18080
        protocol: tcp
        mode: host
    volumes:
      # - "./data:/home/big_data/data" # Your data
      - hdfs-master-data:/home/hadoop/data/nameNode
      - hdfs-master-checkpoint-data:/home/hadoop/data/namesecondary
    deploy:
      mode: global # Required by Docker Swarm to make published ports work with other services
      endpoint_mode: dnsrr # Required to prevent java.net.ConnectException
      placement:
        # Set node labels using `docker node update --label-add role=master <NODE ID>` from swarm manager
        constraints:
          - node.labels.role==master

  # Workers
  worker:
    image: "jwaresolutions/big-data-cluster:0.4.1"
    command: bash -c "/home/big_data/spark-cmd.sh start"
    depends_on:
      - "master-node"
    volumes:
      - hdfs-worker-data:/home/hadoop/data/dataNode
    deploy:
      placement:
        # Set node labels using `docker node update --label-add role=worker <NODE ID>` from swarm manager
        constraints:
          - node.labels.role==worker
      # Deploy N containers for this service
      replicas: 3

  # Jupyter Notebook
  jupyter:
    image: "jupyter/pyspark-notebook"
    #command: bash -c "start-notebook.sh --NotebookApp.password='argon2:$argon2id$v=19$m=10240,t=10,p=8$JdAN3fe9J45NvK/EPuGCvA$O/tbxglbwRpOFuBNTYrymAEH6370Q2z+eS1eF4GM6Do'"
    ports:
      - 10000:8888
    deploy:
      mode: replicated
      replicas: 1
      placement:
        # Set node labels using `docker node update --label-add role=master <NODE ID>` from swarm manager
        constraints:
          - node.labels.role==master

volumes:
  hdfs-master-data:
    external: true
    name: 'hdsf_master_data_swarm'
  hdfs-master-checkpoint-data:
    external: true
    name: 'hdsf_master_checkpoint_data_swarm'
  hdfs-worker-data:
    external: true
    name: 'hdsf_worker_data_swarm'
